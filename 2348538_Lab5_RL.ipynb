{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SoflMwQFxpyF"
   },
   "source": [
    "# Implement Model-Free Prediction & Control With Monte Carlo (MC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-LJ96MTUxrgU"
   },
   "source": [
    "Monte Carlo Prediction\n",
    "The goal is to estimate the value function ùëâ(ùë†) for a given policy ùúã\n",
    "\n",
    "Monte Carlo Control\n",
    "The goal is to optimize the policy by improving the action-value function Q(s,a) iteratively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Z6dYBXJ3xonf"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jEpgRgIIyDA4"
   },
   "source": [
    "Monte-carlo prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OcFMrBCJyyy_"
   },
   "source": [
    "Goal: Learn how good it is to be in a certain state when following a specific policy.\n",
    "\n",
    "* Imagine playing a game (like Blackjack) over and over while following a certain strategy (policy).\n",
    "* Every time you reach a state (e.g., your current cards in Blackjack), you note down the eventual total reward you get by the end of the game.\n",
    "* After enough games, you take the average of all these total rewards for each state.\n",
    "*This average becomes your estimate of how valuable that state is (called V(s)) when using that strategy.\n",
    "Think of it as keeping track of \"how good\" each state is by observing what happens when you visit it many times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "OmMdMwmByAs3"
   },
   "outputs": [],
   "source": [
    "# Monte Carlo Prediction\n",
    "def mc_prediction(policy, env, num_episodes, gamma=1.0):\n",
    "    V = defaultdict(float)\n",
    "    returns = defaultdict(list)\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        # Generate an episode\n",
    "        episode_data = []\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            action = policy(state)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            episode_data.append((state, action, reward))\n",
    "            state = next_state\n",
    "\n",
    "        # Calculate returns\n",
    "        G = 0\n",
    "        for t in reversed(range(len(episode_data))):\n",
    "            state, _, reward = episode_data[t]\n",
    "            G = reward + gamma * G\n",
    "            if state not in [x[0] for x in episode_data[:t]]:\n",
    "                returns[state].append(G)\n",
    "                V[state] = np.mean(returns[state])\n",
    "\n",
    "    return V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ls1g2-jCzmZy"
   },
   "source": [
    "Prediction: Helps estimate how good a situation is under a specific strategy. For example, \"If I have 15 in Blackjack, how likely am I to win?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KQdOv_kTyKgD"
   },
   "source": [
    "Monte-Carlo control"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wneYS-vqzQfN"
   },
   "source": [
    "Goal: Find the best strategy (policy) to play the game, so you win more often.\n",
    "How it works:\n",
    "* Instead of just tracking how good each state is, you now track how good each action is in each state. This is called Q(s,a), the action-value function.\n",
    "* At the start, you try actions randomly (this is called exploration).\n",
    "* Over time, you see which actions lead to the best outcomes and start choosing those actions more often (this is called exploitation).\n",
    "* To keep improving, you mix random actions (exploration) and smart actions (exploitation) using a method called epsilon-greedy: most of the time, you pick the best-known action, but sometimes you try something random to learn more.\n",
    "* After many games, the strategy becomes optimized because it picks the best actions based on what you‚Äôve learned.\n",
    "* Think of this as learning the best way to play the game by trial and error over thousands of games."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Cf0xW12ZyHpe"
   },
   "outputs": [],
   "source": [
    "def mc_control_epsilon_greedy(env, num_episodes, gamma=1.0, epsilon=0.1):\n",
    "    Q = defaultdict(lambda: np.zeros(env.action_space.n))\n",
    "    returns = defaultdict(list)\n",
    "\n",
    "    def epsilon_greedy_policy(state):\n",
    "        if np.random.rand() < epsilon:\n",
    "            return np.random.choice(env.action_space.n)  # Explore\n",
    "        return np.argmax(Q[state])  # Exploit\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        # Generate an episode\n",
    "        episode_data = []\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            action = epsilon_greedy_policy(state)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            episode_data.append((state, action, reward))\n",
    "            state = next_state\n",
    "\n",
    "        # Calculate returns\n",
    "        G = 0\n",
    "        for t in reversed(range(len(episode_data))):\n",
    "            state, action, reward = episode_data[t]\n",
    "            G = reward + gamma * G\n",
    "            if (state, action) not in [(x[0], x[1]) for x in episode_data[:t]]:\n",
    "                returns[(state, action)].append(G)\n",
    "                Q[state][action] = np.mean(returns[(state, action)])\n",
    "\n",
    "    # Derive the policy from Q\n",
    "    policy = {state: np.argmax(actions) for state, actions in Q.items()}\n",
    "    return Q, policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FdMlcfJZzpOe"
   },
   "source": [
    "Control: Helps you figure out the best strategy to win. For example, \"What should I do when I have 15 in Blackjack: hit or stand?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ju1lyeieyR6A",
    "outputId": "a7004520-bab8-4524-d7c9-58103d9e34cb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\noelm\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\gym\\utils\\passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mchoice(env\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mn)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Prediction\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m V \u001b[38;5;241m=\u001b[39m \u001b[43mmc_prediction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrandom_policy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_episodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50000\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mState-Value Function (V):\u001b[39m\u001b[38;5;124m\"\u001b[39m, V)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Control\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[2], line 14\u001b[0m, in \u001b[0;36mmc_prediction\u001b[1;34m(policy, env, num_episodes, gamma)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[0;32m     13\u001b[0m     action \u001b[38;5;241m=\u001b[39m policy(state)\n\u001b[1;32m---> 14\u001b[0m     next_state, reward, done, _ \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[0;32m     15\u001b[0m     episode_data\u001b[38;5;241m.\u001b[39mappend((state, action, reward))\n\u001b[0;32m     16\u001b[0m     state \u001b[38;5;241m=\u001b[39m next_state\n",
      "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 4)"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    env = gym.make(\"Blackjack-v1\")\n",
    "\n",
    "    # Random policy for prediction\n",
    "    def random_policy(state):\n",
    "        return np.random.choice(env.action_space.n)\n",
    "\n",
    "    # Prediction\n",
    "    V = mc_prediction(random_policy, env, num_episodes=50000)\n",
    "    print(\"State-Value Function (V):\", V)\n",
    "\n",
    "    # Control\n",
    "    Q, optimal_policy = mc_control_epsilon_greedy(env, num_episodes=50000)\n",
    "    print(\"Action-Value Function (Q):\", Q)\n",
    "    print(\"Optimal Policy:\", optimal_policy)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyO84Q61pG+07W1rheNHFUSk",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
