{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MDP:\n",
    "    def __init__(self, states, actions,transition_probs, rewards,gamma=0.9):\n",
    "        self.states = states\n",
    "        self.actions = actions\n",
    "        self.transition_prob = transition_prob\n",
    "        self.gamma = gamma\n",
    "        self.rewards = rewards\n",
    " # Value Iteration Algorithm\n",
    "    def value_iteration(self, threshold=0.001):\n",
    "        # Initialize value function for all states to zero\n",
    "        V = np.zeros(len(self.states))\n",
    "        policy = np.zeros(len(self.states), dtype=int)  # Optimal policy\n",
    "\n",
    "        iteration = 0\n",
    "        while True:\n",
    "            delta = 0  # Change in value function\n",
    "            new_V = np.copy(V)\n",
    "\n",
    "            for s in range(len(self.states)):\n",
    "                # Calculate the value of each action in state 's'\n",
    "                action_values = np.zeros(len(self.actions))\n",
    "                for a in range(len(self.actions)):\n",
    "                    action_value = 0\n",
    "                    for next_state in range(len(self.states)):\n",
    "                        prob = self.transition_prob[s, a, next_state]\n",
    "                        reward = self.rewards[s, a, next_state]\n",
    "                        action_value += prob * (reward + self.gamma * V[next_state])\n",
    "                    action_values[a] = action_value\n",
    "\n",
    "                # Update the value of the state to the maximum action value\n",
    "                best_action_value = np.max(action_values)\n",
    "                new_V[s] = best_action_value\n",
    "                policy[s] = np.argmax(action_values)  # Store the best action\n",
    "\n",
    "                # Calculate the maximum difference between old and new value function\n",
    "                delta = max(delta, abs(new_V[s] - V[s]))\n",
    "\n",
    "            V = new_V\n",
    "            iteration += 1\n",
    "\n",
    "            # Stop if the change in value function is below the threshold\n",
    "            if delta < threshold:\n",
    "                break\n",
    "\n",
    "        print(f\"Converged after {iteration} iterations.\")\n",
    "        return V, policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the states, actions, transition probabilities, and rewards\n",
    "states = [0, 1, 2, 3, 4]  \n",
    "actions = [0, 1, 2]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define transition probability matrix for 5 states and 3 actions\n",
    "transition_prob = np.array([\n",
    "    # From state 0\n",
    "    [[0.7, 0.2, 0.1, 0, 0],  # Action 0\n",
    "     [0.3, 0.4, 0.2, 0.1, 0],  # Action 1\n",
    "     [0.2, 0.3, 0.3, 0.1, 0.1]],  # Action 2\n",
    "\n",
    "    # From state 1\n",
    "    [[0.1, 0.6, 0.2, 0.1, 0],  # Action 0\n",
    "     [0.4, 0.4, 0.1, 0.1, 0],  # Action 1\n",
    "     [0.3, 0.3, 0.2, 0.1, 0.1]],  # Action 2\n",
    "\n",
    "    # From state 2\n",
    "    [[0.2, 0.1, 0.6, 0.1, 0],  # Action 0\n",
    "     [0.3, 0.2, 0.3, 0.2, 0],  # Action 1\n",
    "     [0.1, 0.4, 0.4, 0.1, 0]],  # Action 2\n",
    "\n",
    "    # From state 3\n",
    "    [[0.1, 0.2, 0.3, 0.4, 0],  # Action 0\n",
    "     [0.3, 0.3, 0.2, 0.2, 0],  # Action 1\n",
    "     [0.2, 0.3, 0.3, 0.1, 0.1]],  # Action 2\n",
    "\n",
    "    # From state 4\n",
    "    [[0, 0.1, 0.2, 0.3, 0.4],  # Action 0\n",
    "     [0, 0.4, 0.3, 0.2, 0.1],  # Action 1\n",
    "     [0, 0.2, 0.3, 0.4, 0.1]]  # Action 2\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define reward matrix for 5 states and 3 actions\n",
    "rewards = np.array([\n",
    "    # From state 0\n",
    "    [[5, 10, 0, -1, 2],  # Action 0 rewards for moving to each state\n",
    "     [1, 3, 0, 0, 5],    # Action 1 rewards for moving to each state\n",
    "     [0, 1, 2, 3, 4]],   # Action 2 rewards for moving to each state\n",
    "\n",
    "    # From state 1\n",
    "    [[0, 2, 1, -5, 4],   # Action 0 rewards\n",
    "     [1, 2, 0, 0, 3],    # Action 1 rewards\n",
    "     [2, 4, 1, 0, 6]],   # Action 2 rewards\n",
    "\n",
    "    # From state 2\n",
    "    [[0, 1, 10, -2, 5],  # Action 0 rewards\n",
    "     [1, 5, 0, 2, 3],    # Action 1 rewards\n",
    "     [3, 6, 0, 1, 4]],   # Action 2 rewards\n",
    "\n",
    "    # From state 3\n",
    "    [[1, 2, 3, 4, 5],    # Action 0 rewards\n",
    "     [0, 2, -3, 5, 1],   # Action 1 rewards\n",
    "     [2, 3, 4, 0, 6]],   # Action 2 rewards\n",
    "\n",
    "    # From state 4\n",
    "    [[5, 0, 2, 1, 0],    # Action 0 rewards\n",
    "     [4, 1, 0, 3, 2],    # Action 1 rewards\n",
    "     [2, 3, 5, 0, 1]]    # Action 2 rewards\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the MDP\n",
    "mdp = MDP(states, actions, transition_prob, rewards, gamma=0.9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged after 82 iterations.\n"
     ]
    }
   ],
   "source": [
    "# Perform Value Iteration\n",
    "V, policy = mdp.value_iteration()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal Value Function: [48.69541684 44.78697715 49.51660156 45.36088378 43.91231942]\n",
      "Optimal Policy (action to take at each state): [0 2 0 2 2]\n"
     ]
    }
   ],
   "source": [
    "print(\"Optimal Value Function:\", V)\n",
    "print(\"Optimal Policy (action to take at each state):\", policy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
