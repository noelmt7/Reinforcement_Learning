{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lX6N-Ghh2CVu"
   },
   "source": [
    "# **Markov Decision Process (MDP)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "hn2j6ZV2s-He"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YvPPLsxptCfb"
   },
   "outputs": [],
   "source": [
    "class MDP:\n",
    "  def __init__(self, states, actions, rewards, transition_probs, discount_factor=0.9):\n",
    "    self.states = states \n",
    "    self.actions = actions \n",
    "    self.rewards = rewards      \n",
    "    self.transition_probs = transition_probs # dictionary where transition_probs [s][a][s'] is the prob. of moving from state s to state s' with action a\n",
    "    self.discount_factor = discount_factor # discount factor(gamma) for future rewards\n",
    "    self.policy = {s: np.random.choice(actions) for s in states} # random initial policy\n",
    "\n",
    "  def policy_evaluation(self, theta=1e-6):\n",
    "    # Evaluate the current policy to calculate state-value function v(s)\n",
    "    V = {s: 0 for s in self.states} #initialuze state values with 0\n",
    "\n",
    "    while True:\n",
    "      delta = 0\n",
    "      for s in self.states:\n",
    "        v = V[s]\n",
    "        a = self.policy[s]\n",
    "\n",
    "        # Bellman expectation equation\n",
    "        V[s] = sum(self.transition_probs[s][a][s_prime] * (self.rewards[s][a] + self.discount_factor * V[s_prime])\n",
    "        for s_prime in self.states)\n",
    "\n",
    "        delta = max(delta, abs(v - V[s])) # Check for convergence\n",
    "      if delta < theta:\n",
    "        break\n",
    "    return V\n",
    "\n",
    "  def policy_improvement(self, V):\n",
    "    #Improve the current policy using the state values function V(S)\n",
    "\n",
    "    policy_stable = True\n",
    "    for s in self.states:\n",
    "      old_action = self.policy[s]\n",
    "\n",
    "      # Find the action that maximizes expected value\n",
    "      self.policy[s] = max(self.actions, key=lambda a:sum(\n",
    "          self.transition_probs[s][a][s_prime] * (self.rewards[s][a] + self.discount_factor * V[s_prime])\n",
    "          for s_prime in self.states))\n",
    "\n",
    "      if old_action != self.policy[s]:\n",
    "        policy_stable = False\n",
    "    return policy_stable\n",
    "\n",
    "  def policy_iteration(self):\n",
    "    # Perform policy iteration : alternate between policy evaluation and improvement until the policy is stable(optimal)\n",
    "    while True:\n",
    "      V = self.policy_evaluation()\n",
    "      policy_stable = self.policy_improvement(V)\n",
    "      if policy_stable:\n",
    "        return self.policy, V\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kh2Hbx6x0pJ5"
   },
   "source": [
    "### **MDP Components**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "t6G9l14l0mcw",
    "outputId": "cf4485ee-d3ae-4abd-c533-5565e5ebab21"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal Policy: {'s1': 'a2', 's2': 'a2', 's3': 'a1'}\n",
      "Optimal State Values: {'s1': 24.506574930441033, 's2': 10.526312442034197, 's3': 0.0}\n"
     ]
    }
   ],
   "source": [
    "states = ['s1', 's2', 's3']\n",
    "actions = ['a1', 'a2']\n",
    "rewards = {\n",
    "    's1': {'a1': 5, 'a2': 10},\n",
    "    's2': {'a1': -1, 'a2': 2},\n",
    "    's3': {'a1': 0, 'a2': 0}\n",
    "}\n",
    "transition_probs = {\n",
    "    's1': {'a1': {'s1': 0.7, 's2': 0.3, 's3': 0.0}, 'a2': {'s1': 0.4, 's2': 0.6, 's3': 0.0}},\n",
    "    's2': {'a1': {'s1': 0.1, 's2': 0.6, 's3': 0.3}, 'a2': {'s1': 0.0, 's2': 0.9, 's3': 0.1}},\n",
    "    's3': {'a1': {'s1': 0.0, 's2': 0.0, 's3': 1.0}, 'a2': {'s1': 0.0, 's2': 0.0, 's3': 1.0}}\n",
    "}\n",
    "\n",
    "#create MDP instances and perform policy iteration\n",
    "mdp = MDP(states, actions, rewards, transition_probs, discount_factor=0.9)\n",
    "optimal_policy, optimal_value = mdp.policy_iteration()\n",
    "\n",
    "print(\"Optimal Policy:\", optimal_policy)\n",
    "print(\"Optimal State Values:\", optimal_value)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0Phi2pt02Hzw"
   },
   "source": [
    "MDP Initialization:\n",
    "\n",
    "states, actions, rewards, and transition_probs are inputs defining the environment.\n",
    "The initial policy is randomly selected.\n",
    "Policy Evaluation:\n",
    "\n",
    "Iteratively calculates the value function\n",
    "ð‘‰\n",
    "(\n",
    "ð‘ \n",
    ")\n",
    "V(s) for each state based on the current policy.\n",
    "Convergence is reached when changes to\n",
    "ð‘‰\n",
    "(\n",
    "ð‘ \n",
    ")\n",
    "V(s) are smaller than a threshold theta.\n",
    "Policy Improvement:\n",
    "\n",
    "Updates the policy by choosing the action that maximizes expected rewards for each state using\n",
    "ð‘‰\n",
    "(\n",
    "ð‘ \n",
    ")\n",
    "V(s).\n",
    "If no changes are made to the policy, it is considered optimal.\n",
    "Policy Iteration:\n",
    "\n",
    "Alternates between policy evaluation and improvement until the policy converges to the optimal solution.\n",
    "This approach is known as policy iteration for solving MDPs and ensures convergence to the optimal policy and state values."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNEVBIHMcnTeimhzzmfrD7H",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
